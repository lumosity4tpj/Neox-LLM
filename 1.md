几个检查：
1. flash-attention只支持`Ampere`架构，以及fp16和bf16数据格式；
2. q,k,v的dtype必须一致、必须均在cuda device上、last dimension必须均为contiguous
3. head_size必须小于等于256、q的num_heads必须为k/v的num_heads的整数倍（即保证q的num_heads对应一个或多个k/v的num_heads）


mha_fwd:
传入的参数: 
- q: [batch_size, seqlen_q, num_heads, head_size]
- k: [batch_size, seqlen_k, num_heads, head_size]
- v: [batch_size, seqlen_v, num_heads, head_size]
- out: [batch_size, seqlen_q, num_heads, head_size]
- p_dropout: float
- softmax_scale: float
- is_causal: bool
- return_softmax: bool
- gen_: optional


如果head_size不为8的整数，则需要将q,k,v进行补0以满足条件；

round_multiple(x, m)则返回补全后的值a（即a为最小的整除m的数）
head_size_rounded为32的倍数，seqlen_q_rounded和seqlen_k_rounded为128的倍数

softmax_lse: [batch_size, num_heads, seqlen_q]


set_params_fprop为设置fwd的参数：
获得stride: [batch_sride, row_stride, head_stride, 1]







